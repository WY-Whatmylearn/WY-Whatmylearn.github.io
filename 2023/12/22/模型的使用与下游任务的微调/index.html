<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="该篇是的大模型的使用与下游任务的微调，踩在巨人肩膀上的一点小小尝试。目前，很多主流大模型都已开源，这种促进人类进步发展的奉献精神令人敬佩！">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型的使用与下游任务的微调">
<meta property="og:url" content="http://example.com/2023/12/22/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83/index.html">
<meta property="og:site_name" content="WangYing&#39;s Blog">
<meta property="og:description" content="该篇是的大模型的使用与下游任务的微调，踩在巨人肩膀上的一点小小尝试。目前，很多主流大模型都已开源，这种促进人类进步发展的奉献精神令人敬佩！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703141870226.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703149844066.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703143983419.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703144559362.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703144837220.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703146286060.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703146449077.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703147040770.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703223209376.png">
<meta property="og:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703228353443.png">
<meta property="article:published_time" content="2023-12-22T04:02:00.000Z">
<meta property="article:modified_time" content="2024-01-03T02:22:54.187Z">
<meta property="article:author" content="Wang">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703141870226.png">

<link rel="canonical" href="http://example.com/2023/12/22/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>大模型的使用与下游任务的微调 | WangYing's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WangYing's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/22/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.png">
      <meta itemprop="name" content="Wang">
      <meta itemprop="description" content="勤于记录，乐于重温，敢于自嘲，善于修正">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangYing's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大模型的使用与下游任务的微调
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-22 12:02:00" itemprop="dateCreated datePublished" datetime="2023-12-22T12:02:00+08:00">2023-12-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-01-03 10:22:54" itemprop="dateModified" datetime="2024-01-03T10:22:54+08:00">2024-01-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">大模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>该篇是的大模型的使用与下游任务的微调，踩在巨人肩膀上的一点小小尝试。目前，很多主流大模型都已开源，这种促进人类进步发展的奉献精神令人敬佩！</p>
<span id="more"></span>

<h1 id="模型的使用与下游任务的微调"><a href="#模型的使用与下游任务的微调" class="headerlink" title="模型的使用与下游任务的微调"></a>模型的使用与下游任务的微调</h1><h2 id="任务1：多模态大模型-fuyu-8b"><a href="#任务1：多模态大模型-fuyu-8b" class="headerlink" title="任务1：多模态大模型-fuyu_8b"></a>任务1：多模态大模型-fuyu_8b</h2><p>该模型具备强大的图像理解能力，能理解照片、图表、PDF、界面UI等，且处理速度很快，研究团队表示100毫秒内可反馈大图像处理结果。同时它还很“轻巧”，模型规模没超百亿，且没有使用图像编码器。</p>
<h3 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1.模型结构"></a>1.模型结构</h3><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703141870226.png" alt="1703141870226"></p>
<p>模型采用了Transformer的Decoder部分，输入是图片的patch与句子向量。模型的参数量为8B，需要1~2张3090进行训练。</p>
<h3 id="2-模型调用"><a href="#2-模型调用" class="headerlink" title="2.模型调用"></a>2.模型调用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> FuyuProcessor, FuyuForCausalLM</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model and processor</span></span><br><span class="line">model_id = <span class="string">&quot;F:/LLMs/模型2_多模态大模型/fuyu_8b&quot;</span></span><br><span class="line">processor = FuyuProcessor.from_pretrained(model_id)</span><br><span class="line">model=FuyuForCausalLM.from_pretrained(model_id,device_map=<span class="string">&quot;sequebtial&quot;</span>,torch_dtype=torch.bfloat16)    <span class="comment">#device_map有不同设置觉得了模型在GPU上的不同部署。torch_dtype=torch.bfloat16让模型更轻量。</span></span><br></pre></td></tr></table></figure>

<h3 id="3-模型输入"><a href="#3-模型输入" class="headerlink" title="3.模型输入"></a>3.模型输入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text_prompt = <span class="string">&quot;Generate a coco-style caption.\n&quot;</span>  <span class="comment">#文本信息</span></span><br><span class="line">url = <span class="string">&quot;F:/LLMs/模型2_多模态大模型/fuyu_8b/bus.png&quot;</span>  <span class="comment">#图片文件</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(requests.get(url, stream=<span class="literal">True</span>).raw)</span><br><span class="line">inputs = processor(text=text_prompt, images=image, return_tensors=<span class="string">&quot;pt&quot;</span>).to(<span class="string">&quot;cuda:0&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-模型输出"><a href="#4-模型输出" class="headerlink" title="4.模型输出"></a>4.模型输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">generation_output = model.generate(**inputs, max_new_tokens=<span class="number">7</span>)</span><br><span class="line">generation_text=processor.batch_decode(generation_output[:,-<span class="number">7</span>:],skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#此处的7是设置输出的长短，不设置默认20</span></span><br><span class="line"><span class="built_in">print</span>(generation_text)</span><br><span class="line"><span class="keyword">assert</span> generation_text == [<span class="string">&#x27;A blue bus parked on the side of a road.&#x27;</span>]<span class="comment">#评估生成答案与实际的一致性</span></span><br></pre></td></tr></table></figure>

<h3 id="5-使用案例"><a href="#5-使用案例" class="headerlink" title="5.使用案例"></a>5.使用案例</h3><h4 id="1）生成式人脸的判断"><a href="#1）生成式人脸的判断" class="headerlink" title="1）生成式人脸的判断"></a>1）生成式人脸的判断</h4><p>这里使用AI人脸让模型做出判断，问的问题分为两种，一种是让模型做选择题，一种是做判断题。输入、输出以及回答的情况如下所示。</p>
<h5 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703149844066.png" alt="1703149844066"></p>
<p>这是一张由StyleGN2生成的人脸图片。</p>
<h5 id="问答"><a href="#问答" class="headerlink" title="问答"></a><strong>问答</strong></h5><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Q：这张人脸是生成的吗？</span><br><span class="line">A：Yes,the man&#x27;s face is arttificially created.  #回答的比较正常</span><br><span class="line"></span><br><span class="line">Q: 这张人脸是真实存在的还是生成的？</span><br><span class="line">A：A man&#x27;s face is smiling,with his eyes closed,and he is wearing a hat.However, the image appears to be photoshopped, as there is no visible facial hair, and his eyes are closed.</span><br><span class="line">#描述了这张脸，用了似乎是P过的字眼，理由是没有胡子以及眼睛是闭着的（但其实是睁着的）。</span><br></pre></td></tr></table></figure>

<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h5><p>对于生成式人脸的判断，在给模型做判断题时（是或不是），给出了答案。但是也有可能给他一张真实的人脸，他也会回答这是生成的。在给模型做选择题时（是哪一个）,模型首先给出了对整张图片的描述，然后给出了一个较模糊的答案。描述的部分有点偏理实际。因此，在做该任务时，需要数据集进行微调。</p>
<h4 id="2）金融图片问答与描述"><a href="#2）金融图片问答与描述" class="headerlink" title="2）金融图片问答与描述"></a>2）金融图片问答与描述</h4><p>使用模型对金融图片进行描述与题问：</p>
<h5 id="图片-1"><a href="#图片-1" class="headerlink" title="图片"></a><strong>图片</strong></h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703143983419.png" alt="1703143983419"></p>
<h5 id="问答-1"><a href="#问答-1" class="headerlink" title="问答"></a><strong>问答</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q：详细解读这张图片中的信息。</span><br><span class="line">A：The graph displays how China&#x27;s GDP has fluctuated over 20 years.In 1995, China&#x27;s GDP was 100 billion U.S. dollars.By 2012,China&#x27;s GDP had grown to 207.1 billion U.S. dollars.In 2017, China&#x27;s GDP was 207.0 billion U.S. dollars. #一些时间节点时对的，但数据不知道是哪里的，可能是预训练权重中的。</span><br><span class="line"></span><br><span class="line">Q：中央增长率哪一年最高？</span><br><span class="line">A：1995.</span><br></pre></td></tr></table></figure>

<p>可能是这张图片对于模型来说确实复杂了，于是换了一个英文的男女出生率的图表。</p>
<h5 id="图片-2"><a href="#图片-2" class="headerlink" title="图片"></a><strong>图片</strong></h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703144559362.png" alt="1703144559362"></p>
<h5 id="问答："><a href="#问答：" class="headerlink" title="问答："></a>问答：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：男性出生率最高的一年是哪一年？</span><br><span class="line">A：In 2012,the highest life expectancy for men worldwide was 80.2 years.</span><br></pre></td></tr></table></figure>

<p><strong>总结</strong>：对于中午的复杂图表，模型只能抓住主要的时间节点以及主题信息，其他信息解读不出来。对于英文的图表，模型效果明显，回答的较好。</p>
<h4 id="3）图片信息提问与描述"><a href="#3）图片信息提问与描述" class="headerlink" title="3）图片信息提问与描述"></a>3）图片信息提问与描述</h4><p>使用模型对图片中的信息进行回答：</p>
<h5 id="图片1"><a href="#图片1" class="headerlink" title="图片1"></a><strong>图片1</strong></h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703144837220.png" alt="1703144837220"></p>
<h5 id="问答1"><a href="#问答1" class="headerlink" title="问答1"></a><strong>问答1</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q:图片中是哪座桥？</span><br><span class="line">A:The image features the Golden Gate Bridge,which is a prominent suspension bridge spanning the Gloden Gate strait in San Francisco,California.</span><br></pre></td></tr></table></figure>

<h5 id="图片2"><a href="#图片2" class="headerlink" title="图片2"></a><strong>图片2</strong></h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703146286060.png" alt="1703146286060"></p>
<h5 id="问答2"><a href="#问答2" class="headerlink" title="问答2"></a><strong>问答2</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：这个男孩在干嘛？</span><br><span class="line">A：The young man is skateboading in a skate park,peforming tricks and jumps.</span><br></pre></td></tr></table></figure>

<h5 id="图片3"><a href="#图片3" class="headerlink" title="图片3"></a><strong>图片3</strong></h5><p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703146449077.png" alt="1703146449077"></p>
<h5 id="问答3"><a href="#问答3" class="headerlink" title="问答3"></a><strong>问答3</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：这辆车是什么车？</span><br><span class="line">A：The red spors car is a Maserati.</span><br></pre></td></tr></table></figure>

<h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h5><p>在常规的问题提问这一块，该模型表现出了十分优异的性能。</p>
<h2 id="任务2：大模型的微调——新闻分类"><a href="#任务2：大模型的微调——新闻分类" class="headerlink" title="任务2：大模型的微调——新闻分类"></a>任务2：大模型的微调——新闻分类</h2><p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/567922534">保姆级教程，用PyTorch和BERT进行文本分类 - 知乎 (zhihu.com)</a></p>
<p>目前已经有很多预训练完成的大模型，它们很好的解决了各类问题。但是，如果在实际应用中将这些模型适配到合适的场景则需要对模型进行微调。</p>
<h3 id="1-模型结构-1"><a href="#1-模型结构-1" class="headerlink" title="1.模型结构"></a>1.模型结构</h3><p>谷歌就为 NLP 应用程序开发了一个基于 Transformer 的强大的机器学习模型，该模型在不同的基准数据集中优于以前的语言模型。这个模型被称为BERT。</p>
<p>BERT 架构由多个堆叠在一起的 Transformer 编码器组成。每个 Transformer 编码器都封装了两个子层：<strong>一个自注意力层和一个前馈层。</strong></p>
<p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703147040770.png" alt="1703147040770"></p>
<h3 id="2-模型调用-1"><a href="#2-模型调用-1" class="headerlink" title="2.模型调用"></a>2.模型调用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel,BertTokenizer</span><br><span class="line">BERT_PATH = <span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(BERT_PATH)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.tokenize(<span class="string">&#x27;I have a good time, thank you.&#x27;</span>))</span><br><span class="line">bert = BertModel.from_pretrained(BERT_PATH)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;load bert model over&#x27;</span>)</span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;have&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;thank&#x27;</span>, <span class="string">&#x27;you&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line">load bert model over</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span>)</span><br><span class="line">example_text = <span class="string">&#x27;I will watch Memento tonight&#x27;</span></span><br><span class="line">bert_input = tokenizer(example_text,padding=<span class="string">&#x27;max_length&#x27;</span>, </span><br><span class="line">                       max_length = <span class="number">10</span>, </span><br><span class="line">                       truncation=<span class="literal">True</span>,</span><br><span class="line">                       return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(bert_input[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(bert_input[<span class="string">&#x27;token_type_ids&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(bert_input[<span class="string">&#x27;attention_mask&#x27;</span>])</span><br><span class="line">输出：</span><br><span class="line">tensor([[  <span class="number">101</span>,   <span class="number">146</span>,  <span class="number">1209</span>,  <span class="number">2824</span>,  <span class="number">2508</span>, <span class="number">26173</span>,  <span class="number">3568</span>,   <span class="number">102</span>,     <span class="number">0</span>,     <span class="number">0</span>]])</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="3-微调"><a href="#3-微调" class="headerlink" title="3. 微调"></a>3. 微调</h3><h4 id="调用库"><a href="#调用库" class="headerlink" title="调用库"></a>调用库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer，BertModel</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="调用分词器设置标签"><a href="#调用分词器设置标签" class="headerlink" title="调用分词器设置标签"></a>调用分词器设置标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span>)</span><br><span class="line">labels = &#123;<span class="string">&#x27;business&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">          <span class="string">&#x27;entertainment&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">          <span class="string">&#x27;sport&#x27;</span>:<span class="number">2</span>,</span><br><span class="line">          <span class="string">&#x27;tech&#x27;</span>:<span class="number">3</span>,</span><br><span class="line">          <span class="string">&#x27;politics&#x27;</span>:<span class="number">4</span></span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>

<h4 id="制作数据集"><a href="#制作数据集" class="headerlink" title="制作数据集"></a>制作数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, df</span>):</span><br><span class="line">        self.labels = [labels[label] <span class="keyword">for</span> label <span class="keyword">in</span> df[<span class="string">&#x27;category&#x27;</span>]]</span><br><span class="line">        self.texts = [tokenizer(text, </span><br><span class="line">                                padding=<span class="string">&#x27;max_length&#x27;</span>, </span><br><span class="line">                                max_length = <span class="number">512</span>, </span><br><span class="line">                                truncation=<span class="literal">True</span>,</span><br><span class="line">                                return_tensors=<span class="string">&quot;pt&quot;</span>) </span><br><span class="line">                      <span class="keyword">for</span> text <span class="keyword">in</span> df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">classes</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_labels</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Fetch a batch of labels</span></span><br><span class="line">        <span class="keyword">return</span> np.array(self.labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_texts</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Fetch a batch of inputs</span></span><br><span class="line">        <span class="keyword">return</span> self.texts[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        batch_texts = self.get_batch_texts(idx)</span><br><span class="line">        batch_y = self.get_batch_labels(idx)</span><br><span class="line">        <span class="keyword">return</span> batch_texts, batch_y</span><br></pre></td></tr></table></figure>

<h4 id="模型修改：回归任务改为分类任务"><a href="#模型修改：回归任务改为分类任务" class="headerlink" title="模型修改：回归任务改为分类任务"></a>模型修改：回归任务改为分类任务</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BertClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">768</span>, <span class="number">5</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_id, mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=<span class="literal">False</span>)</span><br><span class="line">        dropout_output = self.dropout(pooled_output)</span><br><span class="line">        linear_output = self.linear(dropout_output)</span><br><span class="line">        final_layer = self.relu(linear_output)</span><br><span class="line">        <span class="keyword">return</span> final_layer</span><br></pre></td></tr></table></figure>

<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_data, val_data, learning_rate, epochs</span>):</span><br><span class="line">  <span class="comment"># 通过Dataset类获取训练和验证集</span></span><br><span class="line">    train, val = Dataset(train_data), Dataset(val_data)</span><br><span class="line">    <span class="comment"># DataLoader根据batch_size获取数据，训练时选择打乱样本</span></span><br><span class="line">    train_dataloader = torch.utils.data.DataLoader(train, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_dataloader = torch.utils.data.DataLoader(val, batch_size=<span class="number">2</span>)</span><br><span class="line">  <span class="comment"># 判断是否使用GPU</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">            model = model.cuda()</span><br><span class="line">            criterion = criterion.cuda()</span><br><span class="line">    <span class="comment"># 开始进入训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch_num <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">      <span class="comment"># 定义两个变量，用于存储训练集的准确率和损失</span></span><br><span class="line">            total_acc_train = <span class="number">0</span></span><br><span class="line">            total_loss_train = <span class="number">0</span></span><br><span class="line">      <span class="comment"># 进度条函数tqdm</span></span><br><span class="line">            <span class="keyword">for</span> train_input, train_label <span class="keyword">in</span> tqdm(train_dataloader):</span><br><span class="line"></span><br><span class="line">                train_label = train_label.to(device)</span><br><span class="line">                mask = train_input[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">                input_id = train_input[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">        <span class="comment"># 通过模型得到输出</span></span><br><span class="line">                output = model(input_id, mask)</span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                batch_loss = criterion(output, train_label)</span><br><span class="line">                total_loss_train += batch_loss.item()</span><br><span class="line">                <span class="comment"># 计算精度</span></span><br><span class="line">                acc = (output.argmax(dim=<span class="number">1</span>) == train_label).<span class="built_in">sum</span>().item()</span><br><span class="line">                total_acc_train += acc</span><br><span class="line">        <span class="comment"># 模型更新</span></span><br><span class="line">                model.zero_grad()</span><br><span class="line">                batch_loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">            <span class="comment"># ------ 验证模型 -----------</span></span><br><span class="line">            <span class="comment"># 定义两个变量，用于存储验证集的准确率和损失</span></span><br><span class="line">            total_acc_val = <span class="number">0</span></span><br><span class="line">            total_loss_val = <span class="number">0</span></span><br><span class="line">      <span class="comment"># 不需要计算梯度</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="comment"># 循环获取数据集，并用训练好的模型进行验证</span></span><br><span class="line">                <span class="keyword">for</span> val_input, val_label <span class="keyword">in</span> val_dataloader:</span><br><span class="line">          <span class="comment"># 如果有GPU，则使用GPU，接下来的操作同训练</span></span><br><span class="line">                    val_label = val_label.to(device)</span><br><span class="line">                    mask = val_input[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">                    input_id = val_input[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">  </span><br><span class="line">                    output = model(input_id, mask)</span><br><span class="line"></span><br><span class="line">                    batch_loss = criterion(output, val_label)</span><br><span class="line">                    total_loss_val += batch_loss.item()</span><br><span class="line">                    </span><br><span class="line">                    acc = (output.argmax(dim=<span class="number">1</span>) == val_label).<span class="built_in">sum</span>().item()</span><br><span class="line">                    total_acc_val += acc</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&#x27;&#x27;&#x27;Epochs: <span class="subst">&#123;epoch_num + <span class="number">1</span>&#125;</span> </span></span><br><span class="line"><span class="string">              | Train Loss: <span class="subst">&#123;total_loss_train / <span class="built_in">len</span>(train_data): <span class="number">.3</span>f&#125;</span> </span></span><br><span class="line"><span class="string">              | Train Accuracy: <span class="subst">&#123;total_acc_train / <span class="built_in">len</span>(train_data): <span class="number">.3</span>f&#125;</span> </span></span><br><span class="line"><span class="string">              | Val Loss: <span class="subst">&#123;total_loss_val / <span class="built_in">len</span>(val_data): <span class="number">.3</span>f&#125;</span> </span></span><br><span class="line"><span class="string">              | Val Accuracy: <span class="subst">&#123;total_acc_val / <span class="built_in">len</span>(val_data): <span class="number">.3</span>f&#125;</span>&#x27;&#x27;&#x27;</span>)  </span><br><span class="line">			torch,save(model.state_dicr(),<span class="string">&#x27;./weight.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="主程序"><a href="#主程序" class="headerlink" title="主程序"></a>主程序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#划分数据集</span></span><br><span class="line">    bbc_text_df = pd.read_csv(<span class="string">&#x27;F:/LLMs/dataset/bbc-text.csv&#x27;</span>)</span><br><span class="line">    df = pd.DataFrame(bbc_text_df)</span><br><span class="line">    np.random.seed(<span class="number">112</span>)</span><br><span class="line">    df_train, df_val, df_test = np.split(df.sample(frac=<span class="number">1</span>, random_state=<span class="number">42</span>),</span><br><span class="line">    [<span class="built_in">int</span>(<span class="number">.8</span>*<span class="built_in">len</span>(df)), <span class="built_in">int</span>(<span class="number">.9</span>*<span class="built_in">len</span>(df))])</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(df_train),<span class="built_in">len</span>(df_val), <span class="built_in">len</span>(df_test))</span><br><span class="line">    <span class="comment">#训练</span></span><br><span class="line">    model = BerClassifiee()</span><br><span class="line">    train(model,df_train,df_val,<span class="number">1e-6</span>,<span class="number">5</span>)    </span><br></pre></td></tr></table></figure>

<h3 id="4-测试结果"><a href="#4-测试结果" class="headerlink" title="4.测试结果"></a>4.测试结果</h3><p>这里使用自己在网上找到的新闻来测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer，BertModel</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BertClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BertClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">768</span>, <span class="number">5</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_id, mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=<span class="literal">False</span>)</span><br><span class="line">        dropout_output = self.dropout(pooled_output)</span><br><span class="line">        linear_output = self.linear(dropout_output)</span><br><span class="line">        final_layer = self.relu(linear_output)</span><br><span class="line">        <span class="keyword">return</span> final_layer</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, test_data</span>):</span><br><span class="line">    model = BertClssifier()</span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;F:/LLMs/bert-base-cased&#x27;</span>)</span><br><span class="line">   	text = <span class="string">&#x27;新闻的输入&#x27;</span></span><br><span class="line">    <span class="built_in">input</span> = [tokenizer(text, </span><br><span class="line">             padding=<span class="string">&#x27;max_length&#x27;</span>, </span><br><span class="line">             max_length = <span class="number">512</span>, </span><br><span class="line">             truncation=<span class="literal">True</span>,</span><br><span class="line">             return_tensors=<span class="string">&quot;pt&quot;</span>)]</span><br><span class="line">    </span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)  </span><br><span class="line">	<span class="keyword">if</span> use_cuda:</span><br><span class="line">        model = model.cuda()</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;./weight_path&#x27;</span>),<span class="literal">False</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">         mask = test_input[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">         input_id = test_input[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">1</span>).to(device)</span><br><span class="line">         output = model(input_id, mask)</span><br><span class="line">         <span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p><strong>输入：</strong></p>
<p>On April 4, General Secretary Xi Jinping stressed during his inspection tour of Tsinghua University that he should adhere to the goal of building a world-class university with Chinese characteristics and contribute to serving the prosperity of the country and the rejuvenation of the nation and the happiness of the people. On December 19, General Secretary Xi Jinping presided over the 12rd meeting of the Central Committee for Comprehensively Deepening Reform to deliberate and adopt the “Several Opinions on Deepening the Construction of World-class Universities and First-class Disciplines”.（习大大考察清华以及审议通过《关于深化建设世界一流大学和一流学科的若干意见》）</p>
<p><strong>输出：</strong></p>
<p>0(business)这个判断错误：应该是教育</p>
<p><strong>输入</strong>：</p>
<p>On December 12, Beijing time, the Portland Trail Blazers hosted the Phoenix Suns, and the two teams fought fiercely for four quarters in this game, and finally the Suns lost to the Trail Blazers 20-104.Kevin Durant scored 40 points, plus five assists and four rebounds, Booker had 5 points, seven assists and three rebounds, Allen had 4 points and nine rebounds, and Nurkic had nine points, 26 rebounds, three assists, two steals and two blocks.Simmons had 23 points, 7 assists and 3 rebounds, Grant had 22 points, 4 assists and 2 blocks, Ayton had 16 points, 15 rebounds and 3 assists, and Brogdon had 14 points, 4 rebounds and 4 assists.After the opening of the game, Durant made consecutive shots to help the Suns rebound, and they established a 16-point advantage in the first quarter.However, as the game progressed, Simmons began to exert his power in the second half, leading the team to a 38-20 attack wave in a single quarter, and achieved a comeback in one fell swoop. The two teams battled fiercely for 12 minutes in the final quarter, but the Suns still failed to complete the comeback, and finally lost to the Trail Blazers 104-109.（NBA最新的球赛）</p>
<p><strong>输出：</strong></p>
<p>2（Sport)</p>
<p><strong>输入：</strong></p>
<p>Wall Street noted that among the top ten U.S. bond holding countries and regions announced by TIC, including Chinese mainland, a total of four reductions were reduced in October, Belgium, which ranked seventh, reduced its holdings by $10.316 billion, Luxembourg, which held fourth, reduced its holdings by $282.44 billion, Switzerland, which held ninth place, reduced its holdings by $200.100 billion, and among the six countries and regions that increased their holdings, the United Kingdom and Japan both increased their holdings by more than $&lt;&gt; billion, and the others increased their holdings by less than $&lt;&gt; billion.（华尔街）</p>
<p><strong>输出：</strong></p>
<p>0（business)</p>
<p><strong>总结：</strong>整体看，在提供的数据集上，模型在测试集的准确率高达0.991，在自己搜集的新闻上，3个对了2个。</p>
<p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703223209376.png" alt="1703223209376"></p>
<h2 id="任务3：大模型的微调——影评分类"><a href="#任务3：大模型的微调——影评分类" class="headerlink" title="任务3：大模型的微调——影评分类"></a>任务3：大模型的微调——影评分类</h2><p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/535100411">Huggingface 超详细介绍 - 知乎 (zhihu.com)</a></p>
<h3 id="1-较任务2修改的代码段"><a href="#1-较任务2修改的代码段" class="headerlink" title="1.较任务2修改的代码段"></a>1.较任务2修改的代码段</h3><p>和任务2类似，主要是数据集的不同。该任务使用的数据集为aclImdb（斯坦福大学影评数据集，数据集地址：<a target="_blank" rel="noopener" href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a> 下载后解压，会看到有两个文件夹，test和train）。针对该影评数据集需要修改加载数据集部分的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ImdbDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mode, testNumber=<span class="number">10000</span>, validNumber=<span class="number">5000</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在这里我做了设置，把数据集分成三种形式，可以选择 “train”默认返回全量50000个数据，“test”默认随机返回10000个数据，</span></span><br><span class="line">        <span class="comment"># 如果是选择“valid”模式，随机返回相应数据</span></span><br><span class="line">        <span class="built_in">super</span>(ImdbDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的训练文件夹名称</span></span><br><span class="line">        text_path = [os.path.join(data_base_path, i) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">&quot;test/neg&quot;</span>, <span class="string">&quot;test/pos&quot;</span>]]</span><br><span class="line">        text_path.extend([os.path.join(data_base_path, i) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">&quot;train/neg&quot;</span>, <span class="string">&quot;train/pos&quot;</span>]])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.total_file_path_list = []</span><br><span class="line">            <span class="comment"># 获取训练的全量数据，因为50000个好像也不算大，就没设置返回量，后续做sentence的时候再做处理</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> text_path:</span><br><span class="line">                self.total_file_path_list.extend([os.path.join(i, j) <span class="keyword">for</span> j <span class="keyword">in</span> os.listdir(i)])</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">            self.total_file_path_list = []</span><br><span class="line">            <span class="comment"># 获取测试数据集，默认10000个数据</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> text_path:</span><br><span class="line">                self.total_file_path_list.extend([os.path.join(i, j) <span class="keyword">for</span> j <span class="keyword">in</span> os.listdir(i)])</span><br><span class="line">            self.total_file_path_list = sample(self.total_file_path_list, testNumber)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;valid&quot;</span>:</span><br><span class="line">            self.total_file_path_list = []</span><br><span class="line">            <span class="comment"># 获取验证数据集，默认5000个数据集</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> text_path:</span><br><span class="line">                self.total_file_path_list.extend([os.path.join(i, j) <span class="keyword">for</span> j <span class="keyword">in</span> os.listdir(i)])</span><br><span class="line">            self.total_file_path_list = sample(self.total_file_path_list, validNumber)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 具体要过滤掉哪些字符要看你的文本质量如何</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里定义了一个过滤器，主要是去掉一些没用的无意义字符，标点符号，html字符啥的</span></span><br><span class="line">        fileters = [<span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&quot;&#x27;</span>, <span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;$&#x27;</span>, <span class="string">&#x27;%&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;\(&#x27;</span>, <span class="string">&#x27;\)&#x27;</span>, <span class="string">&#x27;\*&#x27;</span>, <span class="string">&#x27;\+&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;\.&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;:&#x27;</span>, <span class="string">&#x27;;&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;\?&#x27;</span>, <span class="string">&#x27;@&#x27;</span></span><br><span class="line">            , <span class="string">&#x27;\[&#x27;</span>, <span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;\]&#x27;</span>, <span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;`&#x27;</span>, <span class="string">&#x27;\&#123;&#x27;</span>, <span class="string">&#x27;\|&#x27;</span>, <span class="string">&#x27;\&#125;&#x27;</span>, <span class="string">&#x27;~&#x27;</span>, <span class="string">&#x27;\t&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\x97&#x27;</span>, <span class="string">&#x27;\x96&#x27;</span>, <span class="string">&#x27;”&#x27;</span>, <span class="string">&#x27;“&#x27;</span>, ]</span><br><span class="line">        <span class="comment"># sub方法是替换</span></span><br><span class="line">        text = re.sub(<span class="string">&quot;&lt;.*?&gt;&quot;</span>, <span class="string">&quot; &quot;</span>, text, flags=re.S)  <span class="comment"># 去掉&lt;...&gt;中间的内容，主要是文本内容中存在&lt;br/&gt;等内容</span></span><br><span class="line">        text = re.sub(<span class="string">&quot;|&quot;</span>.join(fileters), <span class="string">&quot; &quot;</span>, text, flags=re.S)  <span class="comment"># 替换掉特殊字符，&#x27;|&#x27;是把所有要匹配的特殊字符连在一起</span></span><br><span class="line">        <span class="keyword">return</span> text  <span class="comment"># 返回文本</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        cur_path = self.total_file_path_list[idx]</span><br><span class="line">        <span class="comment"># 返回path最后的文件名。如果path以／或\结尾，那么就会返回空值。即os.path.split(path)的第二个元素。</span></span><br><span class="line">        <span class="comment"># cur_filename返回的是如：“0_3.txt”的文件名</span></span><br><span class="line">        cur_filename = os.path.basename(cur_path)</span><br><span class="line">        <span class="comment"># 标题的形式是：3_4.txt	前面的3是索引，后面的4是分类</span></span><br><span class="line">        <span class="comment"># 如果是小于等于5分的，是负面评论，labei给值维1，否则就是1</span></span><br><span class="line">        labels = []</span><br><span class="line">        sentences = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(cur_filename.split(<span class="string">&quot;_&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]) &lt;= <span class="number">5</span>:</span><br><span class="line">            label = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = <span class="number">1</span></span><br><span class="line">        <span class="comment"># temp.append([label])</span></span><br><span class="line">        labels.append(label)</span><br><span class="line">        text = self.tokenize(<span class="built_in">open</span>(cur_path, encoding=<span class="string">&#x27;UTF-8&#x27;</span>).read().strip())  <span class="comment"># 处理文本中的奇怪符号</span></span><br><span class="line">        sentences.append(text)</span><br><span class="line">        <span class="comment"># 可见我们这里返回了一个list，这个list的第一个值是标签0或者1，第二个值是这句话；</span></span><br><span class="line">        <span class="keyword">return</span> sentences, labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.total_file_path_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>当然，因为是一个2分类任务，所以5分类也需要修改为2分类。</p>
<h3 id="2-测试结果"><a href="#2-测试结果" class="headerlink" title="2.测试结果"></a>2.测试结果</h3><p>最终测试集10000条的准确率为0.998：</p>
<p><img src="/../blog_image/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%BE%E7%89%87/1703228353443.png" alt="1703228353443"></p>
<p>这里输入自己打印的评论：</p>
<p><strong>输入：</strong></p>
<p>最新电视剧：《三大队》：</p>
<p>秦昊这几年真的是开了挂了，迷雾出品的这品质也是一年比一年顶，这次这部《三大队》，不论从演员配置还是剧情水准上都是一如既往的稳，尤其这个“神仙打架”的演员阵容，任谁看了不得大呼一句“卧槽”!</p>
<p>Qin Hao has really been hanging up in the past few years, and the quality of the mist production is also getting better and better year by year, this time this “Three Teams”, both in terms of actor configuration and plot level, is as stable as ever, especially the cast of this “fairy fight”, no one can shout “oh my god”!</p>
<p><strong>输出：</strong></p>
<p>1（表示积极）</p>
<p><strong>输入：</strong></p>
<p>最新电视剧：《三大队》：</p>
<p>秦昊这几年真的是开了挂了，迷雾出品的这品质也是一年比一年顶，这次这部《三大队》，不论从演员配置还是剧情水准上都是一如既往的稳，尤其这个“神仙打架”的演员阵容，任谁看了不得大呼一句“卧槽”!</p>
<p>Qin Hao has really been hanging up in the past few years, and the quality of the mist production is also getting better and better year by year, this time this “Three Teams”, both in terms of actor configuration and plot level, is as stable as ever, especially the cast of this “fairy fight”, no one can shout “oh my god”!</p>
<p><strong>输出：</strong></p>
<p>1（表示积极）</p>
<p><strong>输入：</strong></p>
<p>最新电影：《拿破仑》</p>
<p>法国人争争气呢？别的也就算了，把拿破仑交给英国人拍，几乎每个人物都很无趣——每个历史关键节点的人物动机、决策因素与不可抗拒的历史进程作用在个体时的张力完全都没有体现。</p>
<p>What about the French fighting? Forget anything else, handing over Napoleon to the British, almost every character is boring - the tension between the motivations, decision-making factors, and irresistible historical processes at each key point in history is completely unrepresented.</p>
<p><strong>输出：</strong></p>
<p>0（表示消极)</p>
<p><strong>总结：</strong></p>
<p>在训练中，训练集一共设置了50000条（验证级5000），测试集10000条。较多的数据量使得模型较好的完成了预期中的任务。</p>

    </div>

    
    
    
    
      <div>
    	<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      </div>
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="tag"># 大模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="prev" title="大模型概述">
      <i class="fa fa-chevron-left"></i> 大模型概述
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/12/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B2/" rel="next" title="大模型部署">
      大模型部署 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83"><span class="nav-number">1.</span> <span class="nav-text">模型的使用与下游任务的微调</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A11%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B-fuyu-8b"><span class="nav-number">1.1.</span> <span class="nav-text">任务1：多模态大模型-fuyu_8b</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.模型调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5"><span class="nav-number">1.1.3.</span> <span class="nav-text">3.模型输入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA"><span class="nav-number">1.1.4.</span> <span class="nav-text">4.模型输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B"><span class="nav-number">1.1.5.</span> <span class="nav-text">5.使用案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%EF%BC%89%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E8%84%B8%E7%9A%84%E5%88%A4%E6%96%AD"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">1）生成式人脸的判断</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%87"><span class="nav-number">1.1.5.1.1.</span> <span class="nav-text">图片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%94"><span class="nav-number">1.1.5.1.2.</span> <span class="nav-text">问答</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.1.5.1.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%EF%BC%89%E9%87%91%E8%9E%8D%E5%9B%BE%E7%89%87%E9%97%AE%E7%AD%94%E4%B8%8E%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">2）金融图片问答与描述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%87-1"><span class="nav-number">1.1.5.2.1.</span> <span class="nav-text">图片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%94-1"><span class="nav-number">1.1.5.2.2.</span> <span class="nav-text">问答</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%87-2"><span class="nav-number">1.1.5.2.3.</span> <span class="nav-text">图片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%94%EF%BC%9A"><span class="nav-number">1.1.5.2.4.</span> <span class="nav-text">问答：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%EF%BC%89%E5%9B%BE%E7%89%87%E4%BF%A1%E6%81%AF%E6%8F%90%E9%97%AE%E4%B8%8E%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">3）图片信息提问与描述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%871"><span class="nav-number">1.1.5.3.1.</span> <span class="nav-text">图片1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%941"><span class="nav-number">1.1.5.3.2.</span> <span class="nav-text">问答1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%872"><span class="nav-number">1.1.5.3.3.</span> <span class="nav-text">图片2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%942"><span class="nav-number">1.1.5.3.4.</span> <span class="nav-text">问答2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%873"><span class="nav-number">1.1.5.3.5.</span> <span class="nav-text">图片3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AE%E7%AD%943"><span class="nav-number">1.1.5.3.6.</span> <span class="nav-text">问答3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">1.1.5.3.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A12%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB"><span class="nav-number">1.2.</span> <span class="nav-text">任务2：大模型的微调——新闻分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.模型调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BE%AE%E8%B0%83"><span class="nav-number">1.2.3.</span> <span class="nav-text">3. 微调</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E7%94%A8%E5%BA%93"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">调用库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E7%94%A8%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%BE%E7%BD%AE%E6%A0%87%E7%AD%BE"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">调用分词器设置标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%B6%E4%BD%9C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">制作数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9%EF%BC%9A%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E6%94%B9%E4%B8%BA%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">模型修改：回归任务改为分类任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.2.3.5.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E7%A8%8B%E5%BA%8F"><span class="nav-number">1.2.3.6.</span> <span class="nav-text">主程序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">1.2.4.</span> <span class="nav-text">4.测试结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A13%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94%E5%BD%B1%E8%AF%84%E5%88%86%E7%B1%BB"><span class="nav-number">1.3.</span> <span class="nav-text">任务3：大模型的微调——影评分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%BE%83%E4%BB%BB%E5%8A%A12%E4%BF%AE%E6%94%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E6%AE%B5"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.较任务2修改的代码段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.测试结果</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Wang"
      src="/images/1.png">
  <p class="site-author-name" itemprop="name">Wang</p>
  <div class="site-description" itemprop="description">勤于记录，乐于重温，敢于自嘲，善于修正</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-06 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共93.4k字</span>
</div
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
